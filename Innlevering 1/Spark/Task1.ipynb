{"cells":[{"cell_type":"markdown","source":["#### Names of people in the group\n\nPlease write the names of the people in your group in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85d8965a-c0b4-4802-abb8-05fc5e21ce59","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Name of person Oda Colquhoun\n\nName of person Emil Bj√∏rlykke Berglund"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37c623c5-86c0-48ee-85de-5d514b4b334f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\ndbutils.fs.rm(\"/user\", recurse=True)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"324eb902-51f2-44b0-8ff1-730efac9900c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: False</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: False</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n!pip install -q ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60b530d6-b580-4de2-affb-aad878f4da86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading PySpark modules that we need\nimport unittest\nfrom collections import Counter\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"023f4d10-e729-450c-8d1a-50d494b488d6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 1: defining the schema for the data\nTypically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fcda919c-b51e-4b61-9d61-80cc24f2d15e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Defining a schema for 'badges' table\nbadges_schema = StructType([StructField('UserId', IntegerType(), False),\n                            StructField('Name', StringType(), False),\n                            StructField('Date', TimestampType(), False),\n                            StructField('Class', IntegerType(), False)])\n\n# Defining a schema for 'comments' table\ncomments_schema = StructType([StructField('PostId', IntegerType(), False),\n                            StructField('Score', IntegerType(), False),\n                            StructField('Text', StringType(), False),\n                            StructField('CreationDate', TimestampType(), False),\n                            StructField('UserId', IntegerType(), False)])\n\n# Defining a schema for 'posts' table\nposts_schema = StructType([StructField('Id', IntegerType(), False),\n                           StructField('ParentId', IntegerType(), False),\n                           StructField('PostTypeId', IntegerType(), False),\n                           StructField('CreationDate', TimestampType(), False),\n                           StructField('Score', IntegerType(), False),\n                           StructField('ViewCount', IntegerType(), False),\n                           StructField('Body', StringType(), False),\n                           StructField('OwnerUserId', IntegerType(), False),\n                           StructField('LastActivityDate', TimestampType(), False),\n                           StructField('Title', StringType(), False),\n                           StructField('Tags', StringType(), False),\n                           StructField('AnswerCount', IntegerType(), False),\n                           StructField('CommentCount', IntegerType(), False),\n                           StructField('FavoriteCount', IntegerType(), False),\n                           StructField('ClosedDate', TimestampType(), False)])\n\n# Defining a schema for 'users' table\nusers_schema = StructType([StructField('Id', IntegerType(), False),\n                           StructField('Reputation', IntegerType(), False),\n                           StructField('CreationDate', TimestampType(), False),\n                           StructField('DisplayName', StringType(), False),\n                           StructField('LastAccessDate', TimestampType(), False),\n                           StructField('AboutMe', StringType(), False),\n                           StructField('Views', IntegerType(), False),\n                           StructField('UpVotes', IntegerType(), False),\n                           StructField('DownVotes', IntegerType(), False)])\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab8bdba5-f6d6-43bf-bed9-763d99cfcc91","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 2: implementing two helper functions\nNext, we need to implement two helper functions:\n1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n\nNote that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n\nBTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_csv(source_file: \"path for the CSV file to load\", schema:\"schema for the CSV file being loaded as a DataFrame\") -> DataFrame:\n    df = spark.read.load(source_file,\n                                format=\"csv\",\n                                header=\"true\",\n                                sep='\\t',\n                                schema=schema)\n    return df\n\ndef save_df(df: \"DataFrame to be saved\", table_name: \"name under which the DataFrame will be saved\") -> None:\n    df.write.format(\"parquet\").option(\"parquet.enable.dictionary\", \"true\") \\\n        .option(\"parquet.page.write-checksum.enabled\", \"false\").mode('overwrite') \\\n        .saveAsTable(table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"089f87ff-f2c8-4ac8-8449-cec251c502f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n%load_ext ipython_unittest"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39bc683c-b37a-4842-8bf8-004620b17cca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 3: validating the implementation by running the tests\n\nRun the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n\nNote that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8903e903-7e4f-4c15-99bd-c9129a601fde","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def test_save_dfs(self):\n        #dfs = [(\"/FileStore/tables/users.csv\", users_schema, \"users\"),\n         #     (\"/FileStore/tables/badges.csv\", badges_schema, \"badges\"),\n          #    (\"/FileStore/tables/comments.csv\", comments_schema, \"comments\"),\n           #   (\"/FileStore/tables/posts.csv\", posts_schema, \"posts\")\n            #   ]\n        \n        dfs = [(\"/FileStore/tables/users_1_csv.gz\", users_schema, \"users\"),\n               (\"/FileStore/tables/badges_1_csv.gz\", badges_schema, \"badges\"),\n               (\"/FileStore/tables/comments_1_csv.gz\", comments_schema, \"comments\"),\n               (\"/FileStore/tables/posts_1_csv.gz\", posts_schema, \"posts\")\n               ]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2376f658-2cc7-445b-89ef-781180a4eeab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%%unittest_main\nclass TestTask1(unittest.TestCase):\n   \n    # test 1\n    #result = load_csv(source_file=\"/FileStore/tables/badges.csv\", schema=badges_schema)\n    def test_load_badges(self):\n        result = load_csv(source_file=\"/FileStore/tables/badges__1__csv.gz\", schema=badges_schema)\n        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    \n    # test 2\n    def test_load_posts(self):\n        #result = load_csv(source_file=\"/FileStore/tables/posts.csv\", schema=posts_schema)\n        result = load_csv(source_file=\"/FileStore/tables/posts__1__csv.gz\", schema=posts_schema)\n        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower,\n                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n                                    'ClosedDate']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    \n    # test 3\n    def test_load_comments(self):\n        #result = load_csv(source_file=\"/FileStore/tables/comments.csv\", schema=comments_schema)\n        result = load_csv(source_file=\"/FileStore/tables/comments__1__csv.gz\", schema=comments_schema)\n        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    \n    # test 4\n    def test_load_users(self):\n        #result = load_csv(source_file=\"/FileStore/tables/users.csv\", schema=users_schema)\n        result = load_csv(source_file=\"/FileStore/tables/users__1__csv.gz\", schema=users_schema)\n        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n\n        coulmn_names = Counter(map(str.lower,\n                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n                                    'Views', 'UpVotes', 'DownVotes']))\n        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n                              \"Missing column(s) or column name mismatch\")\n    # test 5\n    def test_save_dfs(self):\n        #dfs = [(\"/FileStore/tables/users.csv\", users_schema, \"users\"),\n         #     (\"/FileStore/tables/badges.csv\", badges_schema, \"badges\"),\n          #    (\"/FileStore/tables/comments.csv\", comments_schema, \"comments\"),\n           #   (\"/FileStore/tables/posts.csv\", posts_schema, \"posts\")\n            #   ]\n        \n        dfs = [(\"/FileStore/tables/users__1__csv.gz\", users_schema, \"users\"),\n               (\"/FileStore/tables/badges__1__csv.gz\", badges_schema, \"badges\"),\n               (\"/FileStore/tables/comments__1__csv.gz\", comments_schema, \"comments\"),\n               (\"/FileStore/tables/posts__1__csv.gz\", posts_schema, \"posts\")\n               ]\n\n        for i in dfs:\n            df = load_csv(source_file=i[0], schema=i[1])\n            save_df(df, i[2])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cd470d59-2571-4b7d-b022-9ee8f7c3e281","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">&lt;ipython_unittest.magics.Status at 0x7f57b266a450&gt;\n&lt;ipython_unittest.magics.Status at 0x7f57b2656290&gt;\n.....\n----------------------------------------------------------------------\nRan 5 tests in 53.167s\n\nOK\nOut[9]: &lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;ipython_unittest.magics.Status at 0x7f57b266a450&gt;\n&lt;ipython_unittest.magics.Status at 0x7f57b2656290&gt;\n.....\n----------------------------------------------------------------------\nRan 5 tests in 53.167s\n\nOK\nOut[9]: &lt;unittest.runner.TextTestResult run=5 errors=0 failures=0&gt;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Subtask 4: answering to questions about Spark related concepts\n\nPlease write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n\n1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n\nWrite your descriptions in the next cell."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0f99b257-8618-4796-aeb0-d9446863c259","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["###### Spark Application: \nA Spark application is a standalone program written using the Apache Spark framework. It consists of a driver program and a set of distributed tasks that are executed on a cluster of computers. A Spark application is designed to process large amounts of data in parallel across a distributed computing environment. The driver program creates a SparkSession, which is responsible for coordinating the execution of tasks on the cluster. Spark applications can be written in various programming languages such as Java, Scala, Python, and R.\n \n###### SparkSession: \nA SparkSession is the entry point to Spark SQL and provides a single point of entry to interact with Spark data processing capabilities. It is used to create DataFrames, DataSets, and RDD. SparkSession also provides configuration options that control various aspects of Spark‚Äôs behavior, such as the number of executor cores and the amount of memory to allocate for each executor. It is created once per application and can be used throughout the lifetime of the application.\n \n###### Transformations: \nIn Spark, Transformations are operations that are applied to a RDD (Resilient Distributed Dataset) to create a new RDD. Transformations are lazily evaluated, meaning that the computation is not executed immediately when the transformation is called. Instead, Spark builds a directed acyclic graph (DAG) of the transformations to be executed, which is then executed when an action is called. Examples of Transformations include map(), filter(), flatMap(), and reduceByKey().\n \n###### Action: \nActions are operations that trigger the execution of a computation on a RDD and return a value or write data to an external storage system. Actions are eagerly evaluated, meaning that the computation is executed immediately when the action is called. Examples of Actions include collect(), count(), reduce(), and save().\n \n###### Lazy Evaluation: \nLazy evaluation is a technique used by Spark to minimize computation and optimize performance. In Spark, Transformations are lazily evaluated, meaning that the computation is not executed immediately when the transformation is called. Instead, Spark builds a DAG of the transformations to be executed, which is then executed when an action is called. This allows Spark to optimize the execution plan and perform computation only when necessary, which can significantly improve performance."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91ec9fda-7848-4f01-8a36-3b82b78be007","inputWidgets":{},"title":""}}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Task1","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":2531618898758148,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1112977678366541}},"nbformat":4,"nbformat_minor":0}
